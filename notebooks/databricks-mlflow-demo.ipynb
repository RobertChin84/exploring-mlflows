{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataBricks MLFlow demo\n",
    "\n",
    "The general premise for the end-to-end for machine learning is to have some order, repeatable, and easy to use/manipulate frameworks to generate results that you can track over time. End-to-End ML flows usually solve all or some of the following challenges\n",
    "\n",
    "* Manage data\n",
    "* Train models\n",
    "* Evaluate models\n",
    "* Deploy models\n",
    "* Make predictions\n",
    "* Monitor predictions\n",
    "(From Michelangelo)\n",
    "\n",
    "Frameworks already exist, that large companies are currently using/experimenting with to solve the above challenges, to name a few of the ones that Alex asked me to look at\n",
    "\n",
    "* [FBLearner Flow](https://code.fb.com/core-data/introducing-fblearner-flow-facebook-s-ai-backbone/)\n",
    "* [Uber's Michelangelo](https://eng.uber.com/michelangelo/)\n",
    "* [Google TFX](http://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform)\n",
    "* [DataBricks MLFlow](https://www.mlflow.org/)\n",
    "\n",
    "Other frameworks that solve some specific challenges\n",
    "* [MITs ModelDB](https://mitdbg.github.io/modeldb/) - Monitor Training/Evaluation/Predictions\n",
    "* [Data Version Control(DVC)](https://dvc.org/) - Manage data\n",
    "\n",
    "All of the above could show us some of the practices\n",
    "\n",
    "This notebook will look at running some examples from [MLFLow](https://www.mlflow.org/docs/latest/index.html). The three demos will cover the three streams that `MLFlow` aims to resolve for the data scientist.\n",
    "\n",
    "* Tracking of projects\n",
    "* Execution of projects\n",
    "* Deployment of projects\n",
    "\n",
    "\n",
    "### How to run the notebook\n",
    "This notebook must be ran by doing the following:\n",
    "* `cd <root>/notebooks`\n",
    "* `pipenv run jupyter notebook`\n",
    "\n",
    "Then you can execute all of the commands in this notebook.\n",
    "\n",
    "remember to run `mlflow ui` in the same directory where you are running the notebooks from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Default': 0,\n",
       " 'Test Basic': 1,\n",
       " 'Test Tensorflow Basic': 3,\n",
       " 'Test Sklearn Basic': 2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will be used to test if an experiment exists/ I am sure there is a get command.\n",
    "list_experiments = mlflow.tracking.list_experiments()\n",
    "experiments = {l.name: l.experiment_id for l in list_experiments}\n",
    "experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of tracking via the MLFlow UI\n",
    "Lets set the experiment and start the run. These steps allows one to partition out the results in a logical fashion on the ui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlflow.tracking.ActiveRun at 0x10b22b320>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = \"Test Basic\"\n",
    "experiment_id = experiments.get(experiment_name)\n",
    "\n",
    "experiment_id = experiment_id if experiment_id else mlflow.create_experiment(experiment_name=experiment_name)\n",
    "\n",
    "mlflow.start_run(source_name=\"a really basic python script\", experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log a parameter (key-value pair)\n",
    "mlflow.log_param(\"param1\", 5)\n",
    "\n",
    "# Log a metric; metrics can be updated throughout the run\n",
    "mlflow.log_metric(\"foo\", 1)\n",
    "mlflow.log_metric(\"foo\", 2)\n",
    "mlflow.log_metric(\"foo\", 3)\n",
    "\n",
    "# Log an artifact (output file)\n",
    "with open(\"output.txt\", \"w\") as f:\n",
    "    f.write(\"Hello world!\")\n",
    "mlflow.log_artifact(\"output.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End run with the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end the run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of running sklearn + deploy prediction API\n",
    "The simple logistic regression model showcases the model training and the built model. This allows fast iteration without losing track of the model.\n",
    "\n",
    "`mlflow.log_metric` wraps around the score object from the standard `LogisticRegression` class. The `mlflow.sklearn.log_model` saves the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlflow.tracking.ActiveRun at 0x11a073e80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = \"Test Sklearn Basic\"\n",
    "experiment_id = experiments.get(experiment_name)\n",
    "experiment_id = experiment_id if experiment_id else mlflow.create_experiment(experiment_name=experiment_name)\n",
    "\n",
    "mlflow.start_run(source_name=\"a really basic sklearn python script\", experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6666666666666666\n",
      "Model saved in run a1505c10fec94379b4d0cbcfea8ec71c\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import mlflow.sklearn\n",
    "X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\n",
    "y = np.array([0, 0, 1, 1, 1, 0])\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "score = lr.score(X, y)\n",
    "print(\"Score: %s\" % score)\n",
    "mlflow.log_metric(\"score\", 0.76)\n",
    "mlflow.log_metric(\"callum\", score)\n",
    "mlflow.sklearn.log_model(lr, \"model\")\n",
    "\n",
    "run_id = mlflow.active_run().info.run_uuid\n",
    "print(\"Model saved in run %s\" % run_id)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `run-id` is given by the last line after \"Model saved in run `<run-id>`\", this id will be used to spin up the flask api so that you can get a prediction with a post request.\n",
    "\n",
    "Note: That `info` only appears on `active_run` after we have executed at least one operation with `mlflow` before that. \n",
    "\n",
    "Please run the following command in a terminal with `mlflow` present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlflow sklearn serve -r c965cd29a231439aad16a618523cdc76 model'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"mlflow sklearn serve -r %s model\" % run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Before running the command above please ensure that `mflow ui` is not in use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have ran the above command in the terminal (any terminal), this will open a flask app locally which we can now hit with the following command\n",
    "`curl -d '[{\"x\": 1}, {\"x\": -1}]' -H 'Content-Type: application/json' -X POST localhost:5000/invocations`\n",
    "\n",
    "The result of which should look something like this\n",
    "`{\"predictions\": [1, 0]}` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of running a tensorflow project\n",
    "The beauty of `MLFlow` is the cross integration with other frameworks, with TFlow being one of them. \n",
    "\n",
    "We will use the example found [here](https://github.com/databricks/mlflow/blob/master/example/tutorial/tensorflow_example.py)\n",
    "\n",
    "Further details for Tensorflow on `MLFlow` can be found on this [blog post](https://databricks.com/blog/2018/07/03/mlflow-0-2-released.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import tensorflow, tracking\n",
    "import shutil\n",
    "import tempfile\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will cause a download of the dataset if you do not have it already locally\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/54/64fmthqj1bj8wyzhctsp93v40000gp/T/tmpsn9zoseu\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/54/64fmthqj1bj8wyzhctsp93v40000gp/T/tmpsn9zoseu', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x108328fd0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "feat_cols = [tf.feature_column.numeric_column(key=\"features\", shape=(x_train.shape[1],))]\n",
    "feat_spec = {\"features\": tf.placeholder(\"float\", name=\"features\", shape=[None, x_train.shape[1]])}\n",
    "hidden_units = [50, 20]\n",
    "steps = 1000\n",
    "regressor = tf.estimator.DNNRegressor(hidden_units=hidden_units, feature_columns=feat_cols)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn({\"features\": x_train}, y_train, num_epochs=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlflow.tracking.ActiveRun at 0x11a073cc0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = \"Test Tensorflow Basic\"\n",
    "experiment_id = experiments.get(experiment_name)\n",
    "experiment_id = experiment_id if experiment_id else mlflow.create_experiment(experiment_name=experiment_name)\n",
    "\n",
    "mlflow.start_run(source_name=\"simple TFlow example with boston housing\", experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/54/64fmthqj1bj8wyzhctsp93v40000gp/T/tmpsn9zoseu/model.ckpt.\n",
      "INFO:tensorflow:loss = 562730.94, step = 1\n",
      "INFO:tensorflow:global_step/sec: 633.353\n",
      "INFO:tensorflow:loss = 8117.486, step = 101 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 922.575\n",
      "INFO:tensorflow:loss = 3434.9106, step = 201 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 807.801\n",
      "INFO:tensorflow:loss = 6780.1943, step = 301 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 849.113\n",
      "INFO:tensorflow:loss = 4860.667, step = 401 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 837.909\n",
      "INFO:tensorflow:loss = 5708.8027, step = 501 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 808.308\n",
      "INFO:tensorflow:loss = 4513.7793, step = 601 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 819.456\n",
      "INFO:tensorflow:loss = 5349.5264, step = 701 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 793.5\n",
      "INFO:tensorflow:loss = 4243.052, step = 801 (0.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 875.636\n",
      "INFO:tensorflow:loss = 3146.377, step = 901 (0.115 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/54/64fmthqj1bj8wyzhctsp93v40000gp/T/tmpsn9zoseu/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2835.092.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-07-20-16:21:51\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/54/64fmthqj1bj8wyzhctsp93v40000gp/T/tmpsn9zoseu/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [100/1000]\n",
      "INFO:tensorflow:Evaluation [200/1000]\n",
      "INFO:tensorflow:Evaluation [300/1000]\n",
      "INFO:tensorflow:Evaluation [400/1000]\n",
      "INFO:tensorflow:Evaluation [500/1000]\n",
      "INFO:tensorflow:Evaluation [600/1000]\n",
      "INFO:tensorflow:Evaluation [700/1000]\n",
      "INFO:tensorflow:Evaluation [800/1000]\n",
      "INFO:tensorflow:Evaluation [900/1000]\n",
      "INFO:tensorflow:Evaluation [1000/1000]\n",
      "INFO:tensorflow:Finished evaluation at 2018-07-20-16:21:52\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 34.821514, global_step = 1000, loss = 4457.154\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /var/folders/54/64fmthqj1bj8wyzhctsp93v40000gp/T/tmpsn9zoseu/model.ckpt-1000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'features': <tf.Tensor 'features:0' shape=(?, 13) dtype=float32>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'features': <tf.Tensor 'features:0' shape=(?, 13) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/54/64fmthqj1bj8wyzhctsp93v40000gp/T/tmpsn9zoseu/model.ckpt-1000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /var/folders/54/64fmthqj1bj8wyzhctsp93v40000gp/T/tmpbchn3vsh/temp-b'1532103712'/saved_model.pb\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/54/64fmthqj1bj8wyzhctsp93v40000gp/T/tmpbchn3vsh/1532103712/variables/variables\n",
      "             0  original_labels\n",
      "0     9.669501              7.2\n",
      "1    21.218803             18.8\n",
      "2    23.562490             19.0\n",
      "3    26.976727             27.0\n",
      "4    24.399288             22.2\n",
      "5    19.568563             24.5\n",
      "6    27.724449             31.2\n",
      "7    24.504230             22.9\n",
      "8    21.696341             20.5\n",
      "9    20.388683             23.2\n",
      "10   12.402156             18.6\n",
      "11   21.090092             14.5\n",
      "12   18.941650             17.8\n",
      "13   33.592880             50.0\n",
      "14   18.168512             20.8\n",
      "15   24.333509             24.3\n",
      "16   24.192371             24.2\n",
      "17   19.332167             19.8\n",
      "18   19.564703             19.1\n",
      "19   24.226803             22.7\n",
      "20   12.895691             12.0\n",
      "21   11.951527             10.2\n",
      "22   20.694916             20.0\n",
      "23   17.935919             18.5\n",
      "24   24.780720             20.9\n",
      "25   21.653294             23.0\n",
      "26   26.283985             27.5\n",
      "27   44.764023             30.1\n",
      "28   12.766986              9.5\n",
      "29   25.469393             22.0\n",
      "..         ...              ...\n",
      "72   22.389894             19.6\n",
      "73   14.743951              7.0\n",
      "74   25.311426             26.4\n",
      "75   22.899906             18.9\n",
      "76   24.554474             20.9\n",
      "77   22.885683             28.1\n",
      "78   32.454807             35.4\n",
      "79   10.245784             10.2\n",
      "80   21.670559             24.3\n",
      "81   33.648582             43.1\n",
      "82   21.547855             17.6\n",
      "83   18.046391             15.4\n",
      "84   22.409527             16.2\n",
      "85   19.631077             27.1\n",
      "86   19.543177             21.4\n",
      "87   20.741829             21.5\n",
      "88   22.411278             22.4\n",
      "89   25.197218             25.0\n",
      "90   22.087534             16.6\n",
      "91   26.965164             18.6\n",
      "92   23.231491             22.0\n",
      "93   28.147270             42.8\n",
      "94   32.498558             35.1\n",
      "95   21.051235             21.5\n",
      "96   35.008976             36.0\n",
      "97   29.483742             21.9\n",
      "98   24.515129             24.1\n",
      "99   32.925350             50.0\n",
      "100  27.435429             26.7\n",
      "101  16.459379             25.0\n",
      "\n",
      "[102 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "with tracking.start_run() as tracked_run:\n",
    "    mlflow.log_param(\"Hidden Units\", hidden_units)\n",
    "    mlflow.log_param(\"Steps\", steps)\n",
    "    regressor.train(train_input_fn, steps=steps)\n",
    "    test_input_fn = tf.estimator.inputs.numpy_input_fn({\"features\": x_test}, y_test, num_epochs=None, shuffle=True)\n",
    "    # Compute mean squared error\n",
    "    mse = regressor.evaluate(test_input_fn, steps=steps)\n",
    "    mlflow.log_metric(\"Mean Square Error\", mse['average_loss'])\n",
    "    # Building a receiver function for exporting\n",
    "    receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feat_spec)\n",
    "    temp = tempfile.mkdtemp()\n",
    "    try:\n",
    "        saved_estimator_path = regressor.export_savedmodel(temp, receiver_fn).decode(\"utf-8\")\n",
    "        # Logging the saved model\n",
    "        tensorflow.log_saved_model(saved_model_dir=saved_estimator_path, signature_def_key=\"predict\", artifact_path=\"model\")\n",
    "        # Reloading the model\n",
    "        pyfunc = tensorflow.load_pyfunc(saved_estimator_path)\n",
    "        df = pd.DataFrame(data=x_test, columns=[\"features\"] * x_train.shape[1])\n",
    "        # Predicting on the loaded Python Function\n",
    "        predict_df = pyfunc.predict(df)\n",
    "        predict_df['original_labels'] = y_test\n",
    "        print(predict_df)\n",
    "    finally:\n",
    "        shutil.rmtree(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the tensorflow serving app using the built model here, but this needs to be looked into further for a more generic way of deploying like the `sklearn serve` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apulu run time.\n",
    "# created custom classifier\n",
    "# docker to docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of a custom classifier\n",
    "A custom classifier opens up the ability to use more complex models and pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlflow.tracking.ActiveRun at 0x112c34978>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = \"Test Custom Classifier\"\n",
    "experiment_id = experiments.get(experiment_name)\n",
    "\n",
    "experiment_id = experiment_id if experiment_id else mlflow.create_experiment(experiment_name=experiment_name)\n",
    "\n",
    "mlflow.start_run(source_name=\"customise classifier\", experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [-2] probability: [0.82530694 0.17469306] class: 0\n",
      "Input: [-1] probability: [0.69342522 0.30657478] class: 0\n",
      "Input: [0] probability: [0.51989912 0.48010088] class: 0\n",
      "Input: [1] probability: [0.34143529 0.65856471] class: 1\n",
      "Input: [2] probability: [0.19885791 0.80114209] class: 1\n",
      "Input: [1] probability: [0.34143529 0.65856471] class: 1\n",
      "Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import pickle\n",
    "\n",
    "class CustomClassifier(ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, base_estimator=LogisticRegression(), threshold=0.5):\n",
    "        self.base_classifier = base_estimator\n",
    "        self._threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.base_classifier.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _print(x, probabilities, pred):\n",
    "        for i, prob in enumerate(probabilities):\n",
    "            print(\"Input: {} probability: {} class: {}\".format(x[i], prob, pred[i]))\n",
    "        \n",
    "    def predict(self, x):\n",
    "        proba = self.base_classifier.predict_proba(x)\n",
    "        predictions = [np.argmax(p) for p in proba]\n",
    "        self._print(x, proba, predictions)\n",
    "        return predictions\n",
    "\n",
    "X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\n",
    "y = np.array([0, 0, 1, 1, 1, 0])\n",
    "lr = CustomClassifier()\n",
    "lr.fit(X, y)\n",
    "score = lr.score(X, y)\n",
    "print(\"Score: %s\" % score)\n",
    "\n",
    "# see code in next block\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did try to run a generic python function/custom classifier above (see below cell), to log to the `mlflow ui` but this was not successful. You can probably look [here](https://www.mlflow.org/docs/latest/models.html#custom-flavors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"model.pkl\", \"wb\") as mf:\n",
    "#     pickle.dump(lr, mf)\n",
    "# mlflow.pyfunc.log_model(lr)\n",
    "# #mlflow.pyfunc.log(lr, \"model\")\n",
    "\n",
    "# run_id = mlflow.active_run().info.run_uuid\n",
    "# print(\"Model saved in run %s\" % run_id)\n",
    "# mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "* Run the code within a docker env to see if we can potentially run a remote server for the `mlflow ui`\n",
    "* See if we can have multiple people working on the same `server` logging models and runs - good for audit purpose\n",
    "* Serve tensorflow models\n",
    "* Can it work on `GCP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
